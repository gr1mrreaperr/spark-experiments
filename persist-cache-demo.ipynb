{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e15428b-41fa-46cb-a0a3-78ae17fea710",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Persist with storage level: MEMORY_AND_DISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbb45d2b-7491-4d43-af0d-3b5acefddc27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n|gender|gender_count|\n+------+------------+\n|Female|        2256|\n|  Male|        2227|\n+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "customer_df= (spark.read.format(\"parquet\")\n",
    "                  .option(\"path\", \"/FileStore/practise-data/*.parquet\")\n",
    "                  .load())\n",
    "\n",
    "filtered_customer= customer_df.filter(F.col(\"city\") != 'boston')\n",
    "\n",
    "customer_group_df = filtered_customer.groupBy(F.col(\"gender\")).agg(F.count(F.col(\"cust_id\")).alias(\"gender_count\"))\n",
    "\n",
    "customer_group_df.persist(StorageLevel.MEMORY_AND_DISK) #persisting to memory and disk\n",
    "\n",
    "customer_group_df.selectExpr(\"*\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d29afa68-6a86-42c8-a99b-ce7d0e9a72ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![ ](files/practise-data/persist_storage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef161424-4adb-43fa-b298-dee67b6c67a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Storage Level: Disk Memory Serialized 1x Replicated <br> This means that data is stored as a stream of bytes inside memory and disk (In our example the data being small was stored completely in memory in serialized form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdede688-948a-4536-8c86-29a0dbec5841",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: DataFrame[gender: string, gender_count: bigint]"
     ]
    }
   ],
   "source": [
    "customer_group_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57e0e073-8782-4af3-83d3-272bc81af4f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Persist without passing any parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0871ef40-88a7-4991-9247-4673e268ab6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n|gender|total_count|\n+------+-----------+\n|Female|       2256|\n|  Male|       2242|\n+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "customer_df_new= (spark.read.format(\"parquet\")\n",
    "                  .option(\"path\", \"/FileStore/practise-data/*.parquet\")\n",
    "                  .load())\n",
    "\n",
    "filtered_customer_new= customer_df_new.filter(F.col(\"city\") != 'seattle')\n",
    "\n",
    "customer_group_df_new = filtered_customer_new.groupBy(F.col(\"gender\")).agg(F.count(F.col(\"cust_id\")).alias(\"total_count\"))\n",
    "\n",
    "customer_group_df_new.persist() #persisting without passing any parameter\n",
    "\n",
    "customer_group_df_new.selectExpr(\"*\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48061d10-4a51-409a-bf41-43f2b205a2d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![](files/practise-data/persist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "705a3a9e-8652-4a7c-b88f-c554d1543889",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Storage Level: Disk Memory Deserialized 1x Replicated <br> Here the data is stored as objects in memory and disk (In our example the data being small was stored completely in memory in deserialized form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42eaac73-ca15-4ee2-9800-c4fd232bbabd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: DataFrame[gender: string, total_count: bigint]"
     ]
    }
   ],
   "source": [
    "customer_group_df_new.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12227e17-ce33-493c-aad1-546858f9ae1d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Using cache to store dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "760fbad9-490e-483f-accd-eae15bce7d9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n|gender| cnt|\n+------+----+\n|Female|2502|\n|  Male|2498|\n+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "customer_cache_df= (spark.read.format(\"parquet\")\n",
    "                  .option(\"path\", \"/FileStore/practise-data/*.parquet\")\n",
    "                  .load())\n",
    "\n",
    "filtered_customer_cache= customer_cache_df.filter((F.col(\"city\") != 'seattle') | (F.col(\"city\") != 'boston'))\n",
    "\n",
    "customer_cache_df = filtered_customer_cache.groupBy(F.col(\"gender\")).agg(F.count(F.col(\"cust_id\")).alias(\"cnt\"))\n",
    "\n",
    "customer_cache_df.cache() #caching dataframe\n",
    "\n",
    "customer_cache_df.selectExpr(\"*\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f84b49a7-9b4d-41e3-bee2-bcd61bd72f4f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![](files/practise-data/cache.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5b67c02-e835-4e40-9b9e-d69376f1c297",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Storage Level: Disk Memory Deserialized 1x Replicated <br> This is similar to persist() where the default nature is to store data in deserialized form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "578a8c09-3d3c-4b13-84f5-1186eb5ffacb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: DataFrame[gender: string, cnt: bigint]"
     ]
    }
   ],
   "source": [
    "customer_cache_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5274bed-4a7b-438e-9113-2e80f594db95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Conclusion: <br> persist(StorageLevel.MEMORY_AND_DISK): Memory and Disk in Deserialized form <br> persist() and cache(): Memory and Disk in Serialized form."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "f83aad91-4622-40f3-93ca-c072bcad1b0c",
     "origId": 3926329049495141,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2442107954193080,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "persist-cache-demo",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
